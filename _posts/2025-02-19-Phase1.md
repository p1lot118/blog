# Phase 1 - Building Web Scaper

In this phase i will be designing a web scraper that will:

1. Bypass security measure of the website
2. Collect the appropriate data
3. Store in the given format

<br>

In this project, **Linkedin.com** will be used as a source for data scraping. 

<br>



```python

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import random

job_list = []
start_index = 0
headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"}

while start_index < 25:
    print(f"--- Fetching results starting at index: {start_index} ---")
    
    # Update the URL with the current start_index
    list_url = f"https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?keywords=Data%20Scientist&location=Melbourne%2C%20Victoria%2C%20Australia&geoId=100992797&start={start_index}"
    
    response = requests.get(list_url, headers=headers, timeout=10)
    list_soup = BeautifulSoup(response.text, "html.parser")
    page_jobs = list_soup.find_all("li")

    if not page_jobs:
        print("No more jobs found. Finishing scrape.")
        break

    id_list = []
    for job in page_jobs:
        base_card_div = job.find("div", {"class": "base-card"})
        if base_card_div:
            job_id = base_card_div.get("data-entity-urn").split(":")[3]
            id_list.append(job_id)

    # PROCESS IDS
    for job_id in id_list:
        try:
            job_url = f"https://www.linkedin.com/jobs-guest/jobs/api/jobPosting/{job_id}"
            job_response = requests.get(job_url, headers=headers, timeout=10)
            
            if job_response.status_code == 200:
                job_soup = BeautifulSoup(job_response.text, "html.parser")
                job_post = {}
                
                company_tag = job_soup.find("a", {"class": "topcard__org-name-link"})
                time_tag = job_soup.find("span", {"class": "posted-time-ago__text"})
                applicant_tag = job_soup.find("span", {"class": "num-applicants__caption"})
                title_tag = job_soup.find("h2", {"class": "topcard__title"})
                
                description_tag = job_soup.find("div", {"class" : "description__text"})

                job_post["job_title"] = title_tag.text.strip() if title_tag else "N/A"
                job_post["company_name"] = company_tag.text.strip() if company_tag else "N/A"
                job_post["time_posted"] = time_tag.text.strip() if time_tag else "N/A"
                job_post["num_applicants"] = applicant_tag.text.strip() if applicant_tag else "N/A"
                job_post["job_id"] = job_id

                if description_tag:
                    job_post["job_description"] = description_tag.get_text(separator=" ").strip()
                else:
                    job_post["job_description"] = "N/A"


                job_list.append(job_post)
                
                time.sleep(random.uniform(1, 3)) 
                
        except Exception as e:
            print(f"Error on {job_id}: {e}")

    # INCREMENT of 25
    start_index += 25
    
    # Save progress every page in case of a crash
    pd.DataFrame(job_list).to_csv('melbourne_datascience_backup.csv', index=False)

# Final Save
jobs_df = pd.DataFrame(job_list)
jobs_df.to_csv('melbourne_datasciencejobs.csv', index=False)

```

<br>

> This is the main code of the scraper.


Before the collection of the data, it makes sures that it bypasses the gates of the website by defining 'User agent header'.

The main purpose of **User agent header** is to introduce your browser to the server so that the server knows how to format the website for your specific device. 

```python

headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"}

```

**Mozilla/5.0**: This is a historical quirk. Decades ago, sites only supported the Mozilla browser. To prevent getting blocked, other browsers started pretending to be Mozilla, and the tradition just stuck. Practically all modern browsers start with this.

**(Windows NT 10.0; Win64; x64)**: This tells the server the user is on a Windows 10 or Windows 11 machine running a 64-bit operating system.

**AppleWebKit/537.36 (KHTML, like Gecko):** This is the browser's "engine" (the software that renders the HTML into what you see on screen). Chrome uses an engine based on WebKit.

**Chrome/119.0.0.0 Safari/537.36**: This explicitly states that the browser is Google Chrome version 119. (It adds "Safari" at the end for more historical compatibility reasons).

If you don't explicitly set a User Agent in your code, Python's requests library will automatically send its own default User-Agent. It looks something like this:

```python
User-Agent: python-requests/2.31.0
```
When LinkedIn's servers see that knocking on their door, their security systems immediately recognize it as an automated Python script, not a real human using a web browser. 

Because websites generally don't want bots scraping their data or eating up their server bandwidth, they will typically respond by:

1. Blocking the request entirely (returning an HTTP 403 Forbidden error).

2. Sending a CAPTCHA challenge that your script can't solve.

3. Feeding your script fake or empty data.


